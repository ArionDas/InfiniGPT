{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading in the text file\n",
    "with open(\"book_clean.txt\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "to_markdown(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InfiniGPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        ## Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        ## Sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoader function\n",
    "\"\"\"\n",
    "batch_size -> The number of samples (sequences) that will be grouped together and processed by the model at each training step. In this case, a batch_size of 4 means 4 sequences will be processed together.\n",
    "max_length -> The maximum length allowed for each sequence. Sequences exceeding this length will be truncated. Here, max_length is set to 256, so sequences will be at most 256 tokens long.\n",
    "stride -> This parameter controls how much the sequence window is shifted when creating new sequences from the text for training. A stride of 128 means the window will be moved 128 positions between consecutive sequences. This can introduce overlap between sequences, which can be helpful for the model to capture longer-range dependencies in the text.\n",
    "shuffle -> This boolean value determines whether the data is shuffled before creating batches. Shuffling helps to reduce the influence of the order in which the data is presented to the model and can improve generalization. Here, shuffle is set to True, which means the data will be shuffled before creating batches.\n",
    "drop_last -> This parameter specifies whether to drop the last incomplete batch if the total number of samples is not perfectly divisible by the batch_size. Dropping the last incomplete batch can simplify training and memory management. Here, drop_last is set to True, so any leftover incomplete batch will be discarded.\n",
    "num_workers -> This parameter defines the number of worker processes to use for data loading. Using multiple workers can improve data loading performance, especially when dealing with large datasets. Here, num_workers is set to 0, which means the data will be loaded using the main process only.\n",
    "\"\"\"\n",
    "def InfiniGPTDataLoader(txt, batch_size=4, max_length=256, stride=128,\n",
    "                        shuffle=False, drop_last=True, num_workers=0):\n",
    "    ## initializing the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    ## creating the dataset\n",
    "    dataset = InfiniGPTDataset(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    ## dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_length = 1024\n",
    "max_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embeddings\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "position_embedding_layer = nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = InfiniGPTDataLoader(txt, batch_size=8, max_length=max_length, stride=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    X, y = batch\n",
    "    \n",
    "    token_embeddings = token_embedding_layer(X)\n",
    "    position_embeddings = position_embedding_layer(torch.arange(max_length))\n",
    "    \n",
    "    input_embeddings = token_embeddings + position_embeddings\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
